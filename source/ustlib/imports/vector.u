import "aligned_storage.u"
import "alloc.u"
import "checked_math.u"
import "container_utils.u"
import "hash.u"
import "minmax.u"
import "random_access_range.u"

namespace ust
{

template</ type T />
class vector
{
public:
	type this_type= vector</T/>;
	type hasher= range_hasher;

public:

	// Factory method for creating vector from given iterator.
	// Input iterator should produce elements equal to "T" or convertible to it.
	template</type RawIterator/>
	fn enable_if( c_reference_tag_count == 0s ) // For now can't process reference tags here - it's too complex.
	from_iterator( iterator</RawIterator/> mut it ) : auto
	{
		var this_type mut result;
		loop
		{
			auto mut next_result= it.next();
			if( next_result.empty() )
			{
				break;
			}
			result.push_back( next_result.try_take() ); // Type conversion is possible here.
		}
		return result;
	}

	template</type T, type Func/>
	fn from_mapped_range( array_view_imut</T/> r, Func& func ) : auto
	{
		var this_type mut result;
		unsafe( result.ensure_capacity( r.size() ) );
		// TODO - optimize this, use unchecked index, use raw memory access directly instead of "push_back".
		for( var size_type mut i= 0s; i < r.size(); ++i )
		{
			result.push_back( func(r[i]) );
		}
		return result;
	}

public:
	// Default constructor.
	fn constructor()= default;

	// Copy constructor.
	fn enable_if( typeinfo</T/>.is_copy_constructible ) constructor( this_type& other )
		( size_(other.size_) )
	{
		if( other.empty() ){ return; }

		unsafe
		{
			capacity_= max( c_first_allocation_size_, other.size_ );
			// Multiplication overflow is not possible here because allocation of source container should not overflow.
			ptr_= byte_ptr_cast</T/>( memory_allocate( capacity_ * c_element_size_ ) );

			for( var size_type mut i(0); i < size_; ++i )
			{
				move_into_uninitialized( $>(ptr_ + i), $>(other.ptr_ + i) ); // Copy-constructor called here.
			}
		}
	}

	// Fill constructor.
	// Disable it for types with mutable references inside, in order to avoid violation of the single mutable reference rule.
	fn
	enable_if( typeinfo</T/>.is_copy_constructible && !typeinfo</T/>.contains_mutable_references )
	constructor( size_type count, T& value ) @( c_param2_pollution )
		( size_(count) )
	{
		if( size_ == 0s ) { return; }

		unsafe
		{
			capacity_= max( c_first_allocation_size_, size_ );
			// Multiplication overflow check is needed here.
			ptr_= byte_ptr_cast</T/>( memory_allocate( mul_overflow_check_halt( capacity_, c_element_size_ ) ) );

			for( var size_type mut i(0); i < size_; ++i )
			{
				move_into_uninitialized( $>(ptr_ + i), value ); // Copy-constructor called here.
			}
		}
	}

	// Constructor with size only (for default-constructible elements).
	fn enable_if( typeinfo</T/>.is_default_constructible ) constructor( size_type count )
		( size_(count) )
	{
		if( size_ == 0s ) { return; }

		unsafe
		{
			capacity_= max( c_first_allocation_size_, size_ );
			// Multiplication overflow check is needed here.
			ptr_= byte_ptr_cast</T/>( memory_allocate( mul_overflow_check_halt( capacity_, c_element_size_ ) ) );

			for( var size_type mut i(0); i < size_; ++i )
			{
				call_default_constructor( $>(ptr_ + i) );
			}
		}
	}

	// Constructor from array. Moves contents of given array into vector.
	template</size_type S/>
	fn conversion_constructor( [ T, S ] mut arr )  @( c_param1_pollution )
		( size_(S) )
	{
		if( size_ == 0s ) { return; }

		unsafe
		{
			capacity_= max( c_first_allocation_size_, size_ );
			// It's impossible to get multiplication overflow here - source array should not occupy more than address space size.
			ptr_= byte_ptr_cast</T/>( memory_allocate( capacity_ * c_element_size_ ) );

			move_into_uninitialized( cast_ref_unsafe</ [ T, S ] />( $>(ptr_) ), move(arr) );
		}
	}

	// Copy assignment operator.
	op enable_if( typeinfo</T/>.is_copy_constructible & typeinfo</T/>.is_copy_assignable ) =( mut this, this_type& other )
	{
		unsafe
		{
			var size_type mut i(0);
			while( i < this.size_ & i < other.size_ )
			{
				$>(this.ptr_ + i)= $>(other.ptr_ + i); // Copy-assignment operator called here.
				++i;
			}

			if( this.size_ < other.size_ ) // Copy-construct tail.
			{
				if( capacity_ < other.size_ )
				{
					capacity_= other.size_;
					// Multiplication overflow check isn't needed here - source vector size is already correct.
					auto num_bytes = capacity_ * c_element_size_;
					if( is_nullptr(ptr_) )
					{
						ptr_= byte_ptr_cast</T/>( memory_allocate( num_bytes ) );
					}
					else
					{
						ptr_= byte_ptr_cast</T/>( memory_reallocate( ptr_cast_to_byte8( ptr_ ), num_bytes ) );
					}
				}

				while( i < other.size_ )
				{
					move_into_uninitialized( $>(ptr_ + i), $>(other.ptr_ + i) ); // Copy-constructor called here.
					++i;
				}
			}
			else if( this.size_ > other.size_ ) // Cut tail.
			{
				while( i < this.size_ )
				{
					call_destructor( $>(ptr_ + i) );
					++i;
				}
			}

			this.size_= other.size_;
		}
	}

	fn destructor()
	{
		for( var size_type mut i(0); i < size_; ++i )
		{
			unsafe( call_destructor( $>(ptr_ + i) ) );
		}
		if( !is_nullptr(ptr_) )
		{
			unsafe( memory_free( ptr_cast_to_byte8( ptr_ ) ) );
		}
	}

	fn size( this ) : size_type
	{
		return size_;
	}

	fn empty( this ) : bool
	{
		return size_ == 0s;
	}

	fn capacity( this ) : size_type
	{
		return capacity_;
	}

	// Indexing

	op[](  mut this, size_type index )
		: T @(c_return_inner_references) & mut @( reference_notation::return_references::param0 )
	{
		halt if( index >= size_ );
		return unsafe( index_unchecked(index) );
	}

	op[]( imut this, size_type index )
		: T @(c_return_inner_references) &imut @( reference_notation::return_references::param0 )
	{
		halt if( index >= size_ );
		return unsafe( index_unchecked(index) );
	}

	fn index_unchecked(  mut this, size_type index ) unsafe
		: T @(c_return_inner_references) & mut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>(ptr_ + index) );
	}

	fn index_unchecked( imut this, size_type index ) unsafe
		: T @(c_return_inner_references) &imut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>(ptr_ + index) );
	}

	// front/back

	fn front(  mut this )
		: T @(c_return_inner_references) & mut @( reference_notation::return_references::param0 )
	{
		halt if(empty());
		return unsafe( front_unchecked() );
	}

	fn front( imut this )
		: T @(c_return_inner_references) &imut @( reference_notation::return_references::param0 )
	{
		halt if(empty());
		return unsafe( front_unchecked() );
	}

	fn back(  mut this )
		: T @(c_return_inner_references) & mut @( reference_notation::return_references::param0 )
	{
		halt if(empty());
		return unsafe( back_unchecked() );
	}

	fn back( imut this )
		: T @(c_return_inner_references) &imut @( reference_notation::return_references::param0 )
	{
		halt if(empty());
		return unsafe( back_unchecked() );
	}

	fn front_unchecked(  mut this ) unsafe
		: T @(c_return_inner_references) & mut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>(ptr_) );
	}

	fn front_unchecked( imut this ) unsafe
		: T @(c_return_inner_references) &imut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>(ptr_) );
	}

	fn back_unchecked(  mut this ) unsafe
		: T @(c_return_inner_references) & mut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>(ptr_ + size_ - 1s) );
	}

	fn back_unchecked( imut this ) unsafe
		: T @(c_return_inner_references) &imut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>(ptr_ + size_ - 1s) );
	}

	// ==
	op enable_if( typeinfo</T/>.is_equality_comparable )
	==( this_type& l, this_type& r ) : bool
	{
		if( l.size() != r.size() )
		{
			return false;
		}

		for( auto mut i= 0s; i < l.size_; ++i )
		{
			if( unsafe( l.index_unchecked(i) ) != unsafe( r.index_unchecked(i) ) )
			{
				return false;
			}
		}

		return true;
	}

	// modificators

	// Append contents of other vector to this, leave "other" vector empty
	fn append( mut this, this_type &mut other ) @( c_param1_pollution )
	{
		auto new_size= add_overflow_check_halt( size_, other.size_ );

		unsafe( ensure_capacity( new_size ) );

		// Just copy memory from other vector to end of this vector.
		unsafe( memory_copy_aligned(
			c_element_alignment_,
			ptr_cast_to_byte8( ptr_ + size_ ),
			ptr_cast_to_byte8( other.ptr_ ),
			c_element_size_ * other.size_ ) );

		// Increase size of this vector, zero size of other vector.
		// Do not touch storage of other vector.
		size_= new_size;
		other.size_= 0s;

		// Do not need to call any destructors here, since values are just moved from one container into another.
	}

	// Append contents of an iterator.
	template</type RawIterator/>
	fn enable_if( c_reference_tag_count == 0s ) // For now can't process reference tags here - it's too complex.
	append( mut this, iterator</RawIterator/> mut it )
	{
		// TODO - if possible, reserve place before this loop.
		loop
		{
			auto mut r= it.next();
			if( r.empty() )
			{
				return;
			}
			push_back( r.try_take() );
		}
	}

	fn swap( mut this, size_type i0, size_type i1 )
	{
		halt if( i0 >= size_ );
		halt if( i1 >= size_ );

		if( i0 == i1 ){ return; }

		unsafe( ust::swap( $>(ptr_ + i0), $>(ptr_ + i1) ) );
	}

	fn push_back( mut this, T mut val ) @( c_param1_pollution )
	{
		unsafe
		{
			// It's impossible to get overflow here, because previous size (and allocation) can't be more than half of address space.
			auto new_size= size_ + 1s;
			ensure_capacity( new_size );

			move_into_uninitialized( $>(ptr_ + size_), move(val) );
			size_= new_size;
		}
	}

	// Push multiple copies of the same element.
	// Disable it for types with mutable references inside, in order to avoid violation of the single mutable reference rule.
	fn
	enable_if( typeinfo</T/>.is_copy_constructible && !typeinfo</T/>.contains_mutable_references )
	push_back( mut this, size_type count, T& val ) @( c_param2_pollution )
	{
		unsafe
		{
			auto new_size= add_overflow_check_halt( size_, count ); // Overflow check is needed here.
			ensure_capacity( new_size );

			for( var size_type mut i(0); i < count; ++i )
			{
				move_into_uninitialized( $>(ptr_ + size_ + i), val );
			}
			size_= new_size;
		}
	}

	// Remove last element. Prefer using this method over "pop_back" if it's not necessary to obtain element removed.
	fn drop_back( mut this )
	{
		halt if(empty());
		--size_;
		unsafe( call_destructor($>(ptr_ + size_)) );
	}

	fn drop_back( mut this, size_type count )
	{
		halt if( count > size_ );

		size_-= count;

		for( var size_type mut i(0); i < count; ++i )
		{
			unsafe( call_destructor( $>(ptr_ + size_ + i) ) );
		}
	}

	// Remove last element and return it.
	fn nodiscard pop_back( mut this ) : T @(c_return_inner_references)
	{
		halt if(empty());
		unsafe
		{
			--size_;
			var T mut result= uninitialized;
			memory_copy_aligned( c_element_alignment_, ptr_cast_to_byte8( $<(result) ), ptr_cast_to_byte8( ptr_ + size_ ), c_element_size_ );
			return result;
		}
	}

	fn nodiscard pop_back_unchecked( mut this ) unsafe : T @(c_return_inner_references)
	{
		unsafe
		{
			--size_;
			var T mut result= uninitialized;
			memory_copy_aligned( c_element_alignment_, ptr_cast_to_byte8( $<(result) ), ptr_cast_to_byte8( ptr_ + size_ ), c_element_size_ );
			return result;
		}
	}

	// Disable it for types with mutable references inside, in order to avoid violation of the single mutable reference rule.
	fn
	enable_if( typeinfo</T/>.is_copy_constructible && !typeinfo</T/>.contains_mutable_references )
	resize( mut this, size_type new_size, T& val ) @( c_param2_pollution )
	{
		// TODO - optimize. Calls to public methods contain unnecessary checks.
		if( new_size > size_ )
		{
			push_back( new_size - size_, val );
		}
		else if( new_size < size_ )
		{
			drop_back( size_ - new_size );
		}
	}

	fn enable_if( typeinfo</T/>.is_default_constructible ) resize( mut this, size_type new_size )
	{
		// TODO - optimize. Calls to public methods contain unnecessary checks.
		if( new_size > size_ )
		{
			unsafe
			{
				ensure_capacity( new_size );

				for( auto mut i= size_; i < new_size; ++i )
				{
					call_default_constructor( $>(ptr_ + i) );
				}
				size_= new_size;
			}
		}
		else if( new_size < size_ )
		{
			drop_back( size_ - new_size );
		}
	}

	// "iter" and "range" methods are declared as zero-param templates to instantiate them lazily and thus avoid instantiation of range and iterator classes where it isn't necessary.

	template<//>
	fn enable_if( c_enable_range )
	range(  mut this ) : random_access_range_mut </T/> @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( random_access_range_mut </T/>( ptr_, size_ ) );
	}

	template<//>
	fn enable_if( c_enable_range )
	range( imut this ) : random_access_range_imut</T/> @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( random_access_range_imut</T/>( ptr_, size_ ) );
	}

	template<//>
	fn enable_if( c_enable_range )
	iter(  mut this ) : auto
	{
		return range().iter();
	}

	template<//>
	fn enable_if( c_enable_range )
	iter( imut this ) : auto
	{
		return range().iter();
	}

	// Access raw data.
	fn data( mut this ) unsafe : $(T)
	{
		return ptr_;
	}

	fn clear( mut this )
	{
		for( var size_type mut i(0); i < size_; ++i )
		{
			unsafe( call_destructor($>(ptr_ +i)) );
		}
		size_= 0s;
	}

	fn shrink_to_fit( mut this )
	{
		if( capacity_ > size_ )
		{
			capacity_= size_;
			unsafe
			{
				if( capacity_ == 0s )
				{
					memory_free( ptr_cast_to_byte8( ptr_ ) );
					ptr_= nullptr</T/>();
				}
				else
				{
					ptr_= byte_ptr_cast</T/>( memory_reallocate( ptr_cast_to_byte8( ptr_ ), capacity_ * c_element_size_ ) );
				}
			}
		}
	}

	// Remove adjacent duplicates.
	// For example, if 2 or more elements in order are equal, only one element is left and others are removed.
	// In case if this vector is sorted or at least all equal elements are grouped together, result will contain only unique elements.
	fn enable_if( typeinfo</T/>.is_equality_comparable )
	remove_adjacent_duplicates( mut this )
	{
		if( size_ <= 1s )
		{
			return;
		}

		var size_type mut dst= 1s, mut src= 1s;
		while( src < size_ )
		{
			if( unsafe( $>( ptr_ + src ) ) == unsafe( $>( ptr_ + (dst - 1s) ) ) )
			{
				// Current element is equal to previous - destroy and skip it.
				unsafe( call_destructor( $>( ptr_ + src ) ) );
			}
			else
			{
				// Current element isn't equal to previous - move it into destination (unless destination pointer is equal source).
				if( dst < src )
				{
					unsafe( memory_copy_aligned(
						c_element_alignment_,
						ptr_cast_to_byte8( ptr_ + dst ),
						ptr_cast_to_byte8( ptr_ + src ),
						c_element_size_ ) );
				}
				++dst;
			}
			++src;
		}

		// Update size - dst is less than size if elements were removed.
		// All destructors were called previously.
		size_= dst;
	}

private:
	auto constexpr c_element_size_= typeinfo</T/>.size_of;
	auto constexpr c_element_alignment_= typeinfo</T/>.align_of;
	auto constexpr c_first_allocation_size_= get_vector_first_allocation_size( c_element_size_ );
	auto constexpr c_reference_tag_count= typeinfo</T/>.reference_tag_count;

	auto c_param1_pollution= reference_notation::pollution::param0_param1_all_inner_references</ c_reference_tag_count />();
	auto c_param2_pollution= reference_notation::pollution::param_n_param_m_all_inner_references</ c_reference_tag_count />( 0u8, 2u8 );
	auto c_return_inner_references= reference_notation::return_inner_references::param0_all_inner_references</ c_reference_tag_count />();

	var bool c_enable_range= typeinfo</T/>.reference_indirection_depth <= 1s && typeinfo</T/>.reference_tag_count <= 1s;

private:
	fn ensure_capacity( mut this, size_type new_size ) unsafe
	{
		unsafe
		{
			if( capacity_ >= new_size )
			{
				return;
			}
			else if( capacity_ == 0s )
			{
				capacity_= max( c_first_allocation_size_, new_size );
				// Multiplication overflow check is needed here.
				ptr_= byte_ptr_cast</T/>( memory_allocate( mul_overflow_check_halt( capacity_, c_element_size_ ) ) );
			}
			else // if( capacity_ < new_size )
			{
				// It's impossible to get capacity overflow here, because "new_size" can't be greater than address space size.
				while( capacity_ < new_size ) { capacity_*= 2s; }
				// Multiplication overflow check is needed here.
				ptr_= byte_ptr_cast</T/>( memory_reallocate( ptr_cast_to_byte8( ptr_ ), mul_overflow_check_halt( capacity_, c_element_size_ ) ) );
			}
		}
	}

private:
	ContainerTag</ T /> container_tag_;

	$(T) ptr_= zero_init;
	size_type size_(0);
	size_type capacity_(0);
}

template</type T, type Func/>
fn make_vector_from_range( random_access_range_imut</T/> r, Func& f ) : auto
{
	type T= typeof( f( r.front() ) );
	return vector</T/>::from_mapped_range( r, f );
}

} // namespace ust

import "alloc.u"
import "assert.u"
import "container_utils.u"
import "iterator.u"
import "hasher.u"
import "hash_apply.u"
import "minmax.u"
import "optional.u"
import "optional_ref.u"

namespace ust
{

// A hash-table implementation for specified keys and values.
// Key value should be equality-comparable and hashable.
// All fundamental types and scalars are hashable.
// Many standard library classes are also hashable.
// Arrays, tuples, simple structs may be also hashed automatically.
// If this doesn't work, you need to implement hashing method for your key type or its parts, like this:
//
// template</type Hasher/>
// fn hash( this, Hasher &mut hasher )
// {
//     // call here "ust::apply_value_to_hasher" for fields needed to be hashed
// }
//
// Iteration order is unspecified and may be different for different instances of the same unordered_map type or even
// for the same instance.
//
template</ type K, type V />
class unordered_map
{
public:
	type key_type= K;
	type value_type= V;
	type this_type= unordered_map</ key_type, value_type />;

	static_assert( typeinfo</K/>.is_copy_constructible, "key type should be copyable" );
	static_assert( typeinfo</K/>.is_copy_assignable, "key type should be copyable" );
	// Can't use type with references inside since we use "optional_ref" for search.
	static_assert( typeinfo</K/>.reference_tag_count == 0s, "this container doesn't support types with references inside" );
	static_assert( typeinfo</V/>.reference_tag_count == 0s, "this container doesn't support types with references inside" );

public:
	// Default constructor.
	fn constructor()= default;

	// Copy constructor.
	fn enable_if( typeinfo</V/>.is_copy_constructible )
	constructor( mut this, this_type &imut other )
	{
		if( other.empty() )
		{
			return;
		}

		unsafe
		{
			var $(unordered_map_impl::ControlTableElement) other_control_table= other.get_control_table();
			var $(TableValue) other_data_table= other.get_data_table();

			for( var size_type mut i(0); i < other.capacity_; ++i )
			{
				if( ( size_type( $>( other_control_table + i ) ) & unordered_map_impl::c_control_element_hash_mask ) == 0s )
				{
					auto &mut table_value= $>( other_data_table + i );
					this.insert( table_value.key_storage, table_value.value_storage );
				}
			}
		}
	}

	fn destructor()
	{
		clear();
		if( !is_nullptr( ptr_ ) )
		{
			unsafe( memory_free( ptr_ ) );
		}
	}

	// Copy assignment operator.
	op enable_if( typeinfo</V/>.is_copy_constructible )
	=( mut this, this_type &imut other )
	{
		this= this_type(other);
	}

	fn size( this ) : size_type
	{
		return size_;
	}

	fn empty( this ) : bool
	{
		return size_ == 0s;
	}

	fn insert( mut this, key_type& key, value_type mut value ) : value_type &mut @( reference_notation::return_references::param0 )
	{
		// Size overflow is inpossible here - previous container size can't be greater than half of address space.
		unsafe
		{
			// Trigger rehashing using not size, but the number of occupied slots, considering one more slot is needed for new value.
			// Do this in order to have not so many occupied slots (values and tombstones),
			// since having too many of them slows-down lookup/insertion.

			// TODO - tune load factor.
			if( (num_occupied_slots_ + 1s) >
				capacity_ * c_max_loading_factor_numerator / c_max_loading_factor_denominator )
			{
				// Rehashing is required, since load factor is exceeded.
				// Trigger it, requesting size for all current elements plus newly-inserted element.
				trigger_rehash( size_ + 1s );
			}

			var $(unordered_map_impl::ControlTableElement) control_table= get_control_table();
			var $(TableValue) data_table= get_data_table();

			auto capacity_mask= capacity_ - 1s;
			auto key_hash= calculate_hash(key);
			auto hash2= unordered_map_impl::get_hash2( key_hash );
			auto mut key_hash_wrapped= unordered_map_impl::get_hash1( key_hash ) & capacity_mask;
			auto mut insert_index= ~0s;
			loop
			{
				var unordered_map_impl::ControlTableElement cell_content= $>( control_table + key_hash_wrapped );
				if( cell_content == unordered_map_impl::c_contents_empty )
				{
					// Finally reached an empty cell. This means that we insert new value.

					++size_;
					++num_occupied_slots_;

					// Insert new value into place of tombstone (if it exists) or in this empty cell.
					if( insert_index == ~0s ){ insert_index= key_hash_wrapped; }

					auto &mut dst_value= $>(data_table + insert_index);

					move_into_uninitialized( dst_value.key_storage, key );
					move_into_uninitialized( dst_value.value_storage, move(value) );
					$>( control_table + insert_index )= unordered_map_impl::ControlTableElement( hash2 );
					return dst_value.value_storage;
				}
				else if( ( size_type(cell_content) & unordered_map_impl::c_control_element_hash_mask ) == 0s )
				{
					auto &mut dst_value= $>(data_table + key_hash_wrapped);
					if( key == dst_value.key_storage )
					{
						dst_value.value_storage= move(value);
						return dst_value.value_storage;
					}
				}
				else
				{
					debug_assert( cell_content == unordered_map_impl::c_contents_value_removed );
					// Remember first tombstone position in order to perform insertion into it.
					// We need to select the first one in order to minimize probing sequence on lookup.
					if( insert_index == ~0s )
					{
						insert_index= key_hash_wrapped;
					}
				}

				key_hash_wrapped= ( key_hash_wrapped + 1s ) & capacity_mask;

				// Eventually we should finish this loop, since we should have at least one empty cell.
			}
		}
	}

	// TODO - maybe add method, like "erase_if_exists"?
	fn erase( mut this, key_type& key ) : value_type
	{
		if( empty() )
		{
			halt;
		}

		// A workaround for cases where "unordered_map" contained many elements but now almost all of them are removed.
		// In such case iteration may be very slow.
		// Perform rehash on erase if size is several times less than capacity, in order to prevent such performance drops.
		// TODO - handle possible overflow.
		// TODO - tune load factor for schrinking.
		if( size_ * c_min_loading_factor_denominator < capacity_ * c_min_loading_factor_numerator )
		{
			unsafe( trigger_rehash( size_ ) );
		}

		var $(unordered_map_impl::ControlTableElement) control_table= get_control_table();
		var $(TableValue) data_table= get_data_table();

		unsafe
		{
			auto capacity_mask= capacity_ - 1s;
			auto key_hash= calculate_hash(key);
			auto hash2= unordered_map_impl::get_hash2( key_hash );
			auto mut key_hash_wrapped= unordered_map_impl::get_hash1( key_hash ) & capacity_mask;
			loop
			{
				var unordered_map_impl::ControlTableElement &mut cell_content= $>( control_table + key_hash_wrapped );
				if( cell_content == unordered_map_impl::c_contents_empty )
				{
					// End search if reached an empty cell.
					halt;
				}
				else if( ( size_type(cell_content) & unordered_map_impl::c_control_element_hash_mask ) == 0s )
				{
					auto &mut table_value= $>(data_table + key_hash_wrapped);
					if( key == table_value.key_storage )
					{
						--size_;
						if( $>( control_table + ( ( key_hash_wrapped + 1s ) & capacity_mask ) ) == unordered_map_impl::c_contents_empty )
						{
							// If next cell is empty, we can mark this cell as empty too without breaking any probing chain.
							// TODO - try to remove also tombstones prior to this cell.
							--num_occupied_slots_;
							cell_content= unordered_map_impl::c_contents_empty;
						}
						else
						{
							// Place a tombstone here to preserve probing chain.
							cell_content= unordered_map_impl::c_contents_value_removed;
						}

						call_destructor( table_value.key_storage );

						var value_type mut r= uninitialized;
						memory_copy_aligned( typeinfo</ typeof(table_value.value_storage) />.align_of, ptr_cast_to_byte8( $<(r) ), ptr_cast_to_byte8( $<(table_value.value_storage) ), typeinfo</ typeof(table_value.value_storage) />.size_of );

						return r;
					}
				}
				else
				{
					// Ignore a tombstone.
					debug_assert( cell_content == unordered_map_impl::c_contents_value_removed );
				}

				key_hash_wrapped= ( key_hash_wrapped + 1s ) & capacity_mask;

				// Eventually we should finish this loop, since we should have at least one empty cell.
			}
		}
	}

	fn drop( mut this, key_type& key )
	{
		erase(key);
	}

	// Check if given key exists in this unordered_map.
	fn exists( this, key_type& key ) : bool
	{
		return !is_nullptr( unsafe( cast_mut(this).find_key( key ) ) );
	}

	fn find( imut this, key_type& key ) : optional_ref_imut</ value_type /> @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( cast_mut(this).find(key) );
	}

	fn find(  mut this, key_type& key ) : optional_ref_mut</ value_type /> @( reference_notation::return_inner_references::param0 )
	{
		unsafe
		{
			var ptr_type table_value_ptr= find_key( key );
			if( !is_nullptr(table_value_ptr) )
			{
				return $>(table_value_ptr).value_storage;
			}
		}

		return null_optional_ref;
	}

	op[](  mut this, key_type& key ) : value_type & mut @( reference_notation::return_references::param0 )
	{
		return find(key).try_deref(); // "try_deref" will halt, if "find" returns empty result.
	}

	op[]( imut this, key_type& key ) : value_type &imut @( reference_notation::return_references::param0 )
	{
		return find(key).try_deref(); // "try_deref" will halt, if "find" returns empty result.
	}

	fn clear( mut this )
	{
		if( is_nullptr(ptr_) ) { return; }

		var $(unordered_map_impl::ControlTableElement) control_table= get_control_table();
		var $(TableValue) data_table= get_data_table();

		unsafe
		{
			for( auto mut i= 0s; i < capacity_; ++i )
			{
				var unordered_map_impl::ControlTableElement &mut cell_content= $>( control_table + i );
				if( ( size_type(cell_content) & unordered_map_impl::c_control_element_hash_mask ) == 0s )
				{
					cell_content= unordered_map_impl::c_contents_value_removed;

					auto &mut table_value= $>( data_table + i );
					call_destructor( table_value.key_storage );
					call_destructor( table_value.value_storage );
				}
			}
		}
		size_= 0s;

		// A workaround for cases where "unordered_map" contained many elements.
		// In such case iteration may be very slow.
		// So, free internal storage to prevent this.
		// TODO - add a possibility to clear map with preserving its storage.
		unsafe
		{
			memory_free( ptr_ );
			ptr_= ust::nullptr</byte8/>();
			capacity_= 0s;
			num_occupied_slots_= 0s;
		}
	}

	// "iter" methods are declared as zero-param templates to instantiate them lazily and thus avoid instantiation of iterator classes where it isn't necessary.

	template<//>
	fn iter( imut this ) : auto
	{
		return wrap_raw_iterator( raw_iterator</false/>( this ) );
	}

	template<//>
	fn iter(  mut this ) : auto
	{
		return wrap_raw_iterator( raw_iterator</true />( this ) );
	}

private:
	// New size must be not less, than current size.
	fn trigger_rehash( mut this, size_type new_size ) unsafe
	{
		debug_assert( new_size >= size_ );

		// We require size for "new_size" elements plus at least one empty slot.
		// Also we shouldn't exceed maximum load factor.
		// Since we perform rounding to nearest power of two, result loading factor is usually less than maximum, which gives some space for further grow.
		var size_type capacity_with_halved_loading_factor= ( new_size + 1s ) * c_max_loading_factor_denominator / c_max_loading_factor_numerator;
		var size_type new_capacity= get_nearest_power_of_two_size( capacity_with_halved_loading_factor );
		unsafe( rehash( new_capacity ) );
	}

	// Performs unconditional rehashing.
	// new capacity should be power of two and be enough to store at least all currently stored values plus one free slot.
	fn rehash( mut this, size_type mut new_capacity ) unsafe
	{
		debug_assert( new_capacity >= size_ + 1s );

		max_assign(
			new_capacity,
			// Use minimum non-zero capacity 8 in order to simplify some calculations.
			// If alignment of the table value is bigger than 8, require capacity equal to this alignment - in order to calculate data table pointer porperly.
			max( 8s, typeinfo</TableValue/>.align_of ) );

		unsafe // reallocate table and rehash.
		{
			// Allocate and initialize new table.

			auto new_capacity_mask= new_capacity - 1s;

			// it's impossible to add more than one key into hash map in one call.
			// Because of that it's impossible to get sizeof * capacity multiplication overflow here.
			// Maximum allocation limit will be reached first.
			auto new_ptr= memory_allocate( new_capacity + new_capacity * typeinfo</TableValue/>.size_of );
			auto new_control_table= byte_ptr_cast</ unordered_map_impl::ControlTableElement />( new_ptr );
			auto new_data_table= byte_ptr_cast</TableValue/>( new_ptr + new_capacity );

			for( auto mut i= 0s; i < new_capacity; ++i )
			{
				$>( new_control_table + i )= unordered_map_impl::c_contents_empty;
			}

			// Move content of old table into new table.
			if( !is_nullptr(ptr_) )
			{
				var $(unordered_map_impl::ControlTableElement) control_table= get_control_table();
				var $(TableValue) data_table= get_data_table();

				for( auto mut i= 0s; i < capacity_; ++i )
				{
					if( ( size_type( $>(control_table + i) ) & unordered_map_impl::c_control_element_hash_mask ) == 0s )
					{
						auto &mut old_value= $>(data_table + i);

						// Insert value into new table.
						auto key_hash= calculate_hash( old_value.key_storage );
						auto hash2= unordered_map_impl::get_hash2( key_hash );
						auto mut key_hash_wrapped= unordered_map_impl::get_hash1( key_hash ) & new_capacity_mask;
						loop
						{
							var unordered_map_impl::ControlTableElement &mut cell_content= $>( new_control_table + key_hash_wrapped );
							if( cell_content == unordered_map_impl::c_contents_empty )
							{
								cell_content= unordered_map_impl::ControlTableElement( hash2 );

								auto &mut new_value= $>( new_data_table + key_hash_wrapped );
								memory_copy_aligned( typeinfo</ key_type   />.align_of, ptr_cast_to_byte8( $<(new_value.key_storage   ) ), ptr_cast_to_byte8( $<( old_value.key_storage   ) ), typeinfo</ key_type   />.size_of );
								memory_copy_aligned( typeinfo</ value_type />.align_of, ptr_cast_to_byte8( $<(new_value.value_storage ) ), ptr_cast_to_byte8( $<( old_value.value_storage ) ), typeinfo</ value_type />.size_of );
								break; // We must find value, bacause new_capacity >= size_
							}
							key_hash_wrapped= ( key_hash_wrapped + 1s ) & new_capacity_mask;
						}
					}
				} // for all old table.

				// All value moved, so, we can free old memory.
				memory_free( ptr_ );
			}

			capacity_= new_capacity;
			ptr_= new_ptr;
			num_occupied_slots_= size_; // After rehashing we have no tombstones, so the number of occupied slots is equal to number of stored elements.
		}
	}

	// Returns bucket for specific key. Returns 'null', if key not found.
	fn find_key( mut this, key_type& key ) : ptr_type
	{
		if( empty() ){ return nullptr</TableValue/>(); }

		var $(unordered_map_impl::ControlTableElement) control_table= get_control_table();
		var $(TableValue) data_table= get_data_table();

		unsafe
		{
			auto capacity_mask= capacity_ - 1s;
			auto key_hash= calculate_hash(key);
			auto hash2= unordered_map_impl::get_hash2( key_hash );
			auto mut key_hash_wrapped= unordered_map_impl::get_hash1( key_hash ) & capacity_mask;
			loop
			{
				var unordered_map_impl::ControlTableElement cell_content= $>( control_table + key_hash_wrapped );
				if( cell_content == unordered_map_impl::c_contents_empty )
				{
					// End search if reached an empty cell.
					return nullptr</TableValue/>();
				}
				else if( ( size_type(cell_content) & unordered_map_impl::c_control_element_hash_mask ) == 0s )
				{
					auto &mut table_value= $>( data_table + key_hash_wrapped );
					if( key == table_value.key_storage )
					{
						return $<( table_value );
					}
				}
				else
				{
					// Ignore a tombstone.
					debug_assert( cell_content == unordered_map_impl::c_contents_value_removed );
				}

				key_hash_wrapped= ( key_hash_wrapped + 1s ) & capacity_mask;

				// Eventually we should finish this loop, since we should have at least one empty cell.
			}
		}
	}

	fn get_control_table( this ) : $(unordered_map_impl::ControlTableElement)
	{
		// Control table goes first.
		return unsafe( byte_ptr_cast</ unordered_map_impl::ControlTableElement />( ptr_ ) );
	}

	fn get_data_table( this ) : $(TableValue)
	{
		// Data table follows after control table.
		// Each element of the control table is just a byte.
		return unsafe( byte_ptr_cast</TableValue/>( ptr_ + capacity_ ) );
	}

	fn calculate_hash( key_type& key ) : size_type
	{
		// TODO - allow to change hasher implementation via a template parameter.
		var default_hasher mut hasher;
		apply_value_to_hasher( hasher, key );
		return hasher.get();
	}

	// Round up to nearest power of two.
	fn get_nearest_power_of_two_size( size_type s ) : size_type
	{
		// TODO - optimize this.
		var size_type mut res= 1s;
		while( res < s )
		{
			res <<= 1u;
		}
		return res;
	}

public:
	// Iterator element classes - for immutable and mutable iteration.
	// They are needed to wrap TableValue nicely.

	class iterator_element_imut
	{
		fn constructor( mut this, TableValue &imut table_value ) @( reference_notation::pollution::param0_param_1_reference )
			( table_value_(table_value) )
			{}

		fn constructor( mut this, iterator_element_imut& other )= default;

		fn key( this ) : key_type & @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.key_storage;
		}

		fn value( this ) : value_type &imut @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.value_storage;
		}

	private:
		TableValue &imut table_value_;
	}

	class iterator_element_mut
	{
		fn constructor( mut this, TableValue & mut table_value ) @( reference_notation::pollution::param0_param_1_reference )
			( table_value_(table_value) )
			{}

		fn constructor( mut this, iterator_element_mut& other )= default;

		fn key( this ) : key_type & @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.key_storage;
		}

		fn value( this ) : value_type & mut @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.value_storage;
		}

	private:
		TableValue & mut table_value_;
	}

	// Iterator for unordered_map.
	// Internally this iterator basically scans hash table and returns contents of non-empty cells.
	// It finishes when it reaches the table end.
	template</ bool is_mutable />
	class raw_iterator
	{
	public:
		// These constructors are safe, since they assume that passed "unordered_map" is always in valid state.

		fn enable_if( is_mutable )
		constructor( this_type & mut m ) @( reference_notation::pollution::param0_param_1_reference )
			( control_table_( m.get_control_table() ), table_( m.get_data_table() ), capacity_(m.capacity_) )
		{}

		fn enable_if( !is_mutable )
		constructor( this_type &imut m ) @( reference_notation::pollution::param0_param_1_reference )
			( control_table_( m.get_control_table() ), table_( m.get_data_table() ), capacity_(m.capacity_) )
		{}

		fn constructor( mut this, raw_iterator</is_mutable/>& other )= default;

		fn nodiscard enable_if( is_mutable )
		next( mut this )
			: optional</ iterator_element_mut /> @( reference_notation::return_inner_references::param0_inner_reference0 )
		{
			advance_to_next_element();
			if( capacity_ == 0s )
			{
				return null_optional;
			}
			unsafe
			{
				var optional</ iterator_element_mut /> res( iterator_element_mut( $>(table_) ) );
				drop_front_unchecked();
				return res;
			}
		}

		fn nodiscard enable_if( !is_mutable )
		next( mut this )
			: optional</ iterator_element_imut /> @( reference_notation::return_inner_references::param0_inner_reference0 )
		{
			advance_to_next_element();
			if( capacity_ == 0s )
			{
				return null_optional;
			}
			unsafe
			{
				var optional</ iterator_element_imut /> res( iterator_element_imut( $>(table_) ) );
				drop_front_unchecked();
				return res;
			}
		}

	private:
		fn advance_to_next_element( mut this )
		{
			while( capacity_ > 0s &&
				( size_type( unsafe( $>(control_table_) ) ) & unordered_map_impl::c_control_element_hash_mask ) != 0s )
			{
				unsafe
				{
					++control_table_;
					++table_;
				}
				--capacity_;
			}
		}

		// Precondition - end is not reached.
		fn drop_front_unchecked( mut this ) unsafe
		{
			unsafe
			{
				++control_table_;
				++table_;
			}
			--capacity_;
		}

	private:
		ReferenceContainerTag</ this_type, is_mutable /> reference_tag_; // Logically hold a reference to the source container.
		$(unordered_map_impl::ControlTableElement) control_table_;
		ptr_type table_;
		size_type capacity_;
	}

private:
	struct TableValue
	{
		// Actually, destructors and constructors not called.
		key_type key_storage;
		value_type value_storage;
	}

	type ptr_type= $(TableValue);

private:
	// Loading factor representing in form numerator/denominator.
	// Greater loading factor means longer lookup chains and thus longer lookup times.
	// Smaller loading factor means faster lookups but more memory consumption.
	// It can be increased if lookup algorithm is optimized.
	var size_type constexpr c_max_loading_factor_numerator= 1s;
	var size_type constexpr c_max_loading_factor_denominator= 2s;

	var size_type constexpr c_min_loading_factor_numerator= 1s;
	var size_type constexpr c_min_loading_factor_denominator= 16s;

private:
	ContainerTag</ tup[ K, V ] /> key_value_tag_;

	$(byte8) ptr_= zero_init; // Control table following by data table.
	size_type size_(0);
	size_type num_occupied_slots_(0); // size + number of tombstones.
	size_type capacity_(0); // Always must be power of two.
}

namespace unordered_map_impl
{

type ControlTableElement= u8;

var ControlTableElement c_contents_empty		( 0b10000000u );
var ControlTableElement c_contents_value_removed( 0b11111111u );
var size_type c_control_element_hash_mask		( 0b10000000u );

fn get_hash1( size_type h ) : size_type
{
	return h >> 7u;
}

fn get_hash2( size_type h ) : size_type
{
	return h & size_type(0x7F);
}

} // namespace unordered_map_impl

} // namespace ust

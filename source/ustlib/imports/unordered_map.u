import "alloc.u"
import "container_utils.u"
import "iterator.u"
import "hasher.u"
import "hash_apply.u"
import "minmax.u"
import "optional.u"
import "optional_ref.u"

namespace ust
{

// A hash-table implementation for specified keys and values.
// Key value should be equality-comparable and hashable.
// All fundamental types and scalars are hashable.
// Many standard library classes are also hashable.
// Arrays, tuples, simple structs may be also hashed automatically.
// If this doesn't work, you need to implement hashing method for your key type or its parts, like this:
//
// template</type Hasher/>
// fn hash( this, Hasher &mut hasher )
// {
//     // call here "ust::apply_value_to_hasher" for fields needed to be hashed
// }
//
// Iteration order is unspecified and may be different for different instances of the same unordered_map type or even
// for the same instance.
//
template</ type K, type V />
class unordered_map
{
public:
	type key_type= K;
	type value_type= V;
	type this_type= unordered_map</ key_type, value_type />;

	static_assert( typeinfo</K/>.is_copy_constructible, "key type should be copyable" );
	static_assert( typeinfo</K/>.is_copy_assignable, "key type should be copyable" );
	// Can't use type with references inside since we use "optional_ref" for search.
	static_assert( typeinfo</K/>.reference_tag_count == 0s, "this container doesn't support types with references inside" );
	static_assert( typeinfo</V/>.reference_tag_count == 0s, "this container doesn't support types with references inside" );

public:
	// Default constructor.
	fn constructor()= default;

	// Copy constructor.
	fn enable_if( typeinfo</V/>.is_copy_constructible )
	constructor( mut this, this_type &imut other )
	{
		unsafe
		{
			for( var size_type mut i(0); i < other.capacity_; ++i )
			{
				auto &mut table_value= $>(other.table_ + i);
				if( table_value.cell_content == unordered_map_impl::CellContent::HaveValue )
				{
					this.insert( table_value.key_storage, table_value.value_storage );
				}
			}
		}
	}

	fn destructor()
	{
		clear();
		if( !is_nullptr(table_) )
		{
			unsafe( memory_free( ptr_cast_to_byte8( table_ ) ) );
		}
	}

	// Copy assignment operator.
	op enable_if( typeinfo</V/>.is_copy_constructible )
	=( mut this, this_type &imut other )
	{
		this= this_type(other);
	}

	fn size( this ) : size_type
	{
		return size_;
	}

	fn empty( this ) : bool
	{
		return size_ == 0s;
	}

	fn insert( mut this, key_type& key, value_type mut value ) : value_type &mut @( reference_notation::return_references::param0 )
	{
		// Size overflow is inpossible here - previous container size can't be greater than half of address space.
		unsafe
		{
			rehash( size_ + 1s );
			auto capacity_mask= capacity_ - 1s;
			auto key_hash= calculate_hash(key);
			auto initial_key_hash_wrapped= key_hash & capacity_mask;
			auto mut key_hash_wrapped= initial_key_hash_wrapped;
			auto mut insert_index= ~0s;
			loop
			{
				auto cell_content= $>(table_ + key_hash_wrapped).cell_content;
				auto next_key_hash_wrapped= ( key_hash_wrapped + 1s ) & capacity_mask;
				if( cell_content == unordered_map_impl::CellContent::Empty || next_key_hash_wrapped == initial_key_hash_wrapped )
				{
					++size_;

					// Insert new value into place of tombstone (if it exists) or in first empty cell.
					if( insert_index == ~0s ){ insert_index= key_hash_wrapped; }

					auto &mut dst_value= $>(table_ + insert_index);

					move_into_uninitialized( dst_value.key_storage, key );
					move_into_uninitialized( dst_value.value_storage, move(value) );
					dst_value.cell_content= unordered_map_impl::CellContent::HaveValue;
					return dst_value.value_storage;
				}
				else if( cell_content == unordered_map_impl::CellContent::ValueRemoved )
				{
					// Remember first tombstone position in order to perform insertion into it.
					// We need to select the first one in order to minimize probing sequence on lookup.
					if( insert_index == ~0s )
					{
						insert_index= key_hash_wrapped;
					}
				}
				else if( cell_content == unordered_map_impl::CellContent::HaveValue )
				{
					auto &mut dst_value= $>(table_ + key_hash_wrapped);
					if( key == dst_value.key_storage )
					{
						dst_value.value_storage= move(value);
						return dst_value.value_storage;
					}
				}

				key_hash_wrapped= next_key_hash_wrapped;
				// We must finish loop, bacause capacity_ >= size_.
			}
		}
	}

	// TODO - maybe add method, like "erase_if_exists"?
	fn erase( mut this, key_type& key ) : value_type
	{
		// A workaround for cases where "unordered_map" contained many elements but now almost all of them are removed.
		// In such case iteration may be very slow.
		// Perform rehash on erase if size is several times less than capacity, in order to prevent such performance drops.
		// TODO - handle possible overflow.
		// TODO - tune load factor for schrinking.
		if( size_ * 16s < capacity_ )
		{
			rehash( size_ + 1s );
		}

		unsafe
		{
			var ptr_type table_value_ptr= find_key( key );
			if( !is_nullptr(table_value_ptr) )
			{
				auto &mut table_value= $>(table_value_ptr);
				--size_;
				table_value.cell_content= unordered_map_impl::CellContent::ValueRemoved;
				call_destructor( table_value.key_storage );
				var value_type mut r= uninitialized;
				memory_copy_aligned( typeinfo</ typeof(table_value.value_storage) />.align_of, ptr_cast_to_byte8( $<(r) ), ptr_cast_to_byte8( $<(table_value.value_storage) ), typeinfo</ typeof(table_value.value_storage) />.size_of );
				return r;
			}
		}
		halt;
	}

	fn drop( mut this, key_type& key )
	{
		erase(key);
	}

	// Check if given key exists in this unordered_map.
	fn exists( this, key_type& key ) : bool
	{
		return !is_nullptr( unsafe( cast_mut(this).find_key( key ) ) );
	}

	fn find( imut this, key_type& key ) : optional_ref_imut</ value_type /> @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( cast_mut(this).find(key) );
	}

	fn find(  mut this, key_type& key ) : optional_ref_mut</ value_type /> @( reference_notation::return_inner_references::param0 )
	{
		unsafe
		{
			var ptr_type table_value_ptr= find_key( key );
			if( !is_nullptr(table_value_ptr) )
			{
				return $>(table_value_ptr).value_storage;
			}
		}

		return null_optional_ref;
	}

	op[](  mut this, key_type& key ) : value_type & mut @( reference_notation::return_references::param0 )
	{
		return find(key).try_deref(); // "try_deref" will halt, if "find" returns empty result.
	}

	op[]( imut this, key_type& key ) : value_type &imut @( reference_notation::return_references::param0 )
	{
		return find(key).try_deref(); // "try_deref" will halt, if "find" returns empty result.
	}

	fn clear( mut this )
	{
		if( is_nullptr(table_) ) { return; }
		unsafe
		{
			for( auto mut i= 0s; i < capacity_; ++i )
			{
				auto &mut table_value= $>(table_ + i);
				if( table_value.cell_content == unordered_map_impl::CellContent::HaveValue )
				{
					table_value.cell_content= unordered_map_impl::CellContent::ValueRemoved;
					call_destructor( table_value.key_storage );
					call_destructor( table_value.value_storage );
				}
			}
		}
		size_= 0s;

		// A workaround for cases where "unordered_map" contained many elements.
		// In such case iteration may be very slow.
		// So, free internal storage to prevent this.
		// TODO - add a possibility to clear map with preserving its storage.
		unsafe
		{
			memory_free( ptr_cast_to_byte8( table_ ) );
			table_= ust::nullptr</TableValue/>();
			capacity_= 0s;
		}
	}

	// "iter" methods are declared as zero-param templates to instantiate them lazily and thus avoid instantiation of iterator classes where it isn't necessary.

	template<//>
	fn iter( imut this ) : auto
	{
		return wrap_raw_iterator( raw_iterator</false/>( this ) );
	}

	template<//>
	fn iter(  mut this ) : auto
	{
		return wrap_raw_iterator( raw_iterator</true />( this ) );
	}

private:
	fn rehash( mut this, size_type expected_size )
	{
		if( expected_size <= size_ ) { return; }

		auto mut new_capacity= max( capacity_, 2s );
		while( !( ( expected_size << 1u ) <= new_capacity ) ) { new_capacity<<= 1u; }
		while( !( ( expected_size << 2u ) >  new_capacity ) ) { new_capacity>>= 1u; }

		if( new_capacity == capacity_ ) { return; }

		unsafe // reallocate table and rehash.
		{
			auto new_capacity_mask= new_capacity - 1s;

			// it's impossible to add more than one key into hash map in one call.
			// Because of that it's impossible to get sizeof * capacity multiplication overflow here.
			// Maximum allocation limit will be reached first.
			// Allocate and initialize new table.
			auto new_table= byte_ptr_cast</TableValue/>( memory_allocate( new_capacity * typeinfo</TableValue/>.size_of ) );
			for( auto mut i= 0s; i < new_capacity; ++i )
			{
				$>(new_table + i).cell_content= unordered_map_impl::CellContent::Empty;
			}

			// Move content of old table into new table.
			if( !is_nullptr(table_) )
			{
				for( auto mut i= 0s; i < capacity_; ++i )
				{
					auto &mut old_value= $>(table_ + i);
					if( old_value.cell_content == unordered_map_impl::CellContent::Empty || old_value.cell_content == unordered_map_impl::CellContent::ValueRemoved )
					{}
					else if( old_value.cell_content == unordered_map_impl::CellContent::HaveValue )
					{
						// Insert value into new table.
						auto key_hash= calculate_hash( old_value.key_storage );
						auto mut key_hash_wrapped= key_hash & new_capacity_mask;
						loop
						{
							auto &mut new_value= $>(new_table + key_hash_wrapped);
							if( new_value.cell_content == unordered_map_impl::CellContent::Empty )
							{
								new_value.cell_content= unordered_map_impl::CellContent::HaveValue;
								memory_copy_aligned( typeinfo</ key_type   />.align_of, ptr_cast_to_byte8( $<(new_value.key_storage   ) ), ptr_cast_to_byte8( $<( old_value.key_storage   ) ), typeinfo</ key_type   />.size_of );
								memory_copy_aligned( typeinfo</ value_type />.align_of, ptr_cast_to_byte8( $<(new_value.value_storage ) ), ptr_cast_to_byte8( $<( old_value.value_storage ) ), typeinfo</ value_type />.size_of );
								break; // We must find value, bacause new_capacity >= size_
							}
							key_hash_wrapped= ( key_hash_wrapped + 1s ) & new_capacity_mask;
						}
					}
					else{ halt; }
				} // for all old table.

				// All value moved, so, we can free old memory.
				memory_free( ptr_cast_to_byte8( table_) );
			}
			capacity_= new_capacity;
			table_= new_table;
		}
	}

	// Returns bucket for specific key. Returns 'null', if key not found.
	fn find_key( mut this, key_type& key ) : ptr_type
	{
		if( empty() ){ return nullptr</TableValue/>(); }

		unsafe
		{
			auto capacity_mask= capacity_ - 1s;
			auto key_hash= calculate_hash(key);
			auto mut key_hash_wrapped= key_hash & capacity_mask;
			auto key_hash_wrapped_finish= key_hash_wrapped;
			loop
			{
				auto &mut table_value= $>(table_ + key_hash_wrapped);
				if( table_value.cell_content == unordered_map_impl::CellContent::Empty ) { break; }
				else if( table_value.cell_content == unordered_map_impl::CellContent::ValueRemoved ) {}
				else if( table_value.cell_content == unordered_map_impl::CellContent::HaveValue )
				{
					if( key == table_value.key_storage )
					{
						return $<( table_value );
					}
				}

				key_hash_wrapped= ( key_hash_wrapped + 1s ) & capacity_mask;
				if( key_hash_wrapped == key_hash_wrapped_finish ){ break; } // Value not found in whole table.
			}
		}
		return nullptr</TableValue/>();
	}

	fn calculate_hash( key_type& key ) : size_type
	{
		// TODO - allow to change hasher implementation via a template parameter.
		var default_hasher mut hasher;
		apply_value_to_hasher( hasher, key );
		return hasher.get();
	}

public:
	// Iterator element classes - for immutable and mutable iteration.
	// They are needed to wrap TableValue nicely.

	class iterator_element_imut
	{
		fn constructor( mut this, TableValue &imut table_value ) @( reference_notation::pollution::param0_param_1_reference )
			( table_value_(table_value) )
			{}

		fn constructor( mut this, iterator_element_imut& other )= default;

		fn key( this ) : key_type & @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.key_storage;
		}

		fn value( this ) : value_type &imut @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.value_storage;
		}

	private:
		TableValue &imut table_value_;
	}

	class iterator_element_mut
	{
		fn constructor( mut this, TableValue & mut table_value ) @( reference_notation::pollution::param0_param_1_reference )
			( table_value_(table_value) )
			{}

		fn constructor( mut this, iterator_element_mut& other )= default;

		fn key( this ) : key_type & @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.key_storage;
		}

		fn value( this ) : value_type & mut @( reference_notation::return_references::param0_inner_reference0 )
		{
			return table_value_.value_storage;
		}

	private:
		TableValue & mut table_value_;
	}

	// Iterator for unordered_map.
	// Internally this iterator basically scans hash table and returns contents of non-empty cells.
	// It finishes when it reaches the table end.
	template</ bool is_mutable />
	class raw_iterator
	{
	public:
		// These constructors are safe, since they assume that passed "unordered_map" is always in valid state.

		fn enable_if( is_mutable )
		constructor( this_type & mut m ) @( reference_notation::pollution::param0_param_1_reference )
			( table_(m.table_), capacity_(m.capacity_) )
		{}

		fn enable_if( !is_mutable )
		constructor( this_type &imut m ) @( reference_notation::pollution::param0_param_1_reference )
			( table_(m.table_), capacity_(m.capacity_) )
		{}

		fn constructor( mut this, raw_iterator</is_mutable/>& other )= default;

		fn nodiscard enable_if( is_mutable )
		next( mut this )
			: optional</ iterator_element_mut /> @( reference_notation::return_inner_references::param0_inner_reference0 )
		{
			advance_to_next_element();
			if( capacity_ == 0s )
			{
				return null_optional;
			}
			unsafe
			{
				var optional</ iterator_element_mut /> res( iterator_element_mut( $>(table_) ) );
				drop_front_unchecked();
				return res;
			}
		}

		fn nodiscard enable_if( !is_mutable )
		next( mut this )
			: optional</ iterator_element_imut /> @( reference_notation::return_inner_references::param0_inner_reference0 )
		{
			advance_to_next_element();
			if( capacity_ == 0s )
			{
				return null_optional;
			}
			unsafe
			{
				var optional</ iterator_element_imut /> res( iterator_element_imut( $>(table_) ) );
				drop_front_unchecked();
				return res;
			}
		}

	private:
		fn advance_to_next_element( mut this )
		{
			while( capacity_ > 0s && unsafe( $>(table_) ).cell_content != unordered_map_impl::CellContent::HaveValue )
			{
				unsafe{ ++table_; }
				--capacity_;
			}
		}

		// Precondition - end is not reached.
		fn drop_front_unchecked( mut this ) unsafe
		{
			unsafe{ ++table_; }
			--capacity_;
		}

	private:
		ReferenceContainerTag</ this_type, is_mutable /> reference_tag_; // Logically hold a reference to the source container.
		ptr_type table_;
		size_type capacity_;
	}

private:
	struct TableValue
	{
		unordered_map_impl::CellContent cell_content;
		// Actually, destructors and constructors not called.
		key_type key_storage;
		value_type value_storage;
	}

	type ptr_type= $(TableValue);

private:
	ContainerTag</ tup[ K, V ] /> key_value_tag_;

	ptr_type table_= zero_init;
	size_type size_(0);
	size_type capacity_(0); // Always must be power of two.
}

namespace unordered_map_impl
{

enum CellContent
{
	Empty,
	HaveValue,
	ValueRemoved
}

} // namespace unordered_map_impl

} // namespace ust

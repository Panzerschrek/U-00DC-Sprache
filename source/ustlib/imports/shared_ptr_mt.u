import "alloc.u"
import "atomic.u"
import "container_utils.u"
import "hash_apply.u"
import "polymorph.u"
import "rwlock_impl.u"

namespace ust
{

template</ type T, bool is_nullable, bool is_mutable />
class shared_ptr_mt_base
{
	static_assert( typeinfo</T/>.reference_tag_count == 0s, "this container doesn't support types with references inside" );

	// It has no sense to use multithreaded shared pointer for types, with singlethreaded shared pointers inside.
	// Use singlethreaded shared pointer instead.
	static_assert( !non_sync</T/>, "using non_sync types inside multithreaded shared pointers is not allowed" );

public:
	// Default constructor. Exists only for nullable shared_ptr_mt.
	fn enable_if(is_nullable) constructor()= default;

	// Construct with value.
	fn constructor( T mut value )
	{
		unsafe
		{
			// Allocate counter and object in single allocation.
			// Counter goes first, than goes the object itself (with proper alignment).
			// The only disadvantage of this approach is that memory isn't freed if a weak_ptr instance is still alive.
			auto constexpr counter_size= typeinfo</ shared_ptr_mt_impl::counter />.size_of;
			auto constexpr object_size= typeinfo</T/>.size_of;
			auto constexpr object_alignment= typeinfo</T/>.align_of;
			auto constexpr object_offset= (counter_size + (object_alignment - 1s)) / object_alignment * object_alignment;
			static_assert(object_offset % object_alignment == 0s);
			auto constexpr allocation_size= object_offset + object_size;

			var $(byte8) mem= memory_allocate( allocation_size );

			counter_= byte_ptr_cast</ shared_ptr_mt_impl::counter />( mem );
			$>(counter_).use_count_total = 1u;
			$>(counter_).use_count_strong= 1u;

			rwlock_impl::rwlock_init( $>(counter_).rwlock );

			value_= byte_ptr_cast</T/>( mem + object_offset );
			move_into_uninitialized( $>(value_), move(value) );
		}
	}

	// Constructor for nullable ptr from non-nullable ptr.
	fn enable_if( is_nullable ) conversion_constructor( mut this, shared_ptr_mt_base</T, false, is_mutable/>& ptr )
	{
		unsafe
		{
			value_= ptr.get_value_ptr();
			counter_= ptr.get_counter_ptr();
			atomic_inc( $>(counter_).use_count_total  );
			atomic_inc( $>(counter_).use_count_strong );
		}
	}

	// Constructor for immutable ptr from mutable ptr.
	fn enable_if( !is_mutable ) conversion_constructor( mut this, shared_ptr_mt_base</T, is_nullable, true/>& ptr )
	{
		if( !ptr.empty() )
		{
			unsafe
			{
				value_= ptr.get_value_ptr();
				counter_= ptr.get_counter_ptr();
				atomic_inc( $>(counter_).use_count_total  );
				atomic_inc( $>(counter_).use_count_strong );
			}
		}
	}

	// Constructor for immutable nullable ptr from mutable non-nullable ptr.
	fn enable_if( is_nullable && !is_mutable ) conversion_constructor( mut this, shared_ptr_mt_base</T, false, true/>& ptr )
	{
		unsafe
		{
			value_= ptr.get_value_ptr();
			counter_= ptr.get_counter_ptr();
			atomic_inc( $>(counter_).use_count_total  );
			atomic_inc( $>(counter_).use_count_strong );
		}
	}

	// Copy constructor.
	fn constructor( mut this, this_type &imut other )
	{
		if( !other.empty() )
		{
			unsafe
			{
				value_= other.value_;
				counter_= other.counter_;
				atomic_inc( $>(counter_).use_count_total  );
				atomic_inc( $>(counter_).use_count_strong );
			}
		}
	}

	// Construct from shared_ptr_mt of compatible type.
	template</ type U />
	fn enable_if( !same_type</T, U/> && is_ancestor_for</T, U/>() )
	conversion_constructor( mut this, shared_ptr_mt_base</U, is_nullable, is_mutable/>& ptr )
	{
		if( !ptr.empty() )
		{
			unsafe
			{
				value_= $<( cast_ref</T/>( $>( ptr.get_value_ptr() ) ) );
				counter_= ptr.get_counter_ptr();
				// It's almost impossible to get counter overflow here - so, ignore such possibility.
				atomic_inc( $>(counter_).use_count_total  );
				atomic_inc( $>(counter_).use_count_strong );
			}
		}
	}

	fn destructor()
	{
		if( empty() ) { return; }
		unsafe
		{
			if( atomic_dec( $>(counter_).use_count_strong ) == 1u )
			{
				call_destructor( $>(value_) );
			}
			if( atomic_dec( $>(counter_).use_count_total ) == 1u )
			{
				rwlock_impl::rwlock_destroy( $>(counter_).rwlock );
				// Free counter together with initial allocated object.
				memory_free( ptr_cast_to_byte8( counter_ ) );
			}
		}
	}

	// Copy-assignment operator.
	op=( mut this, this_type &imut other )
	{
		// Call copy constructor for other and move-assign copy to this.
		this= this_type(other);
	}

	fn enable_if(is_nullable) reset( mut this )
	{
		// Move value ot out "this", call destructor for it, construct default value in place of "this".
		take(this);
	}

	fn reset( mut this, T mut value )
	{
		this= this_type( move(value) );
	}

	fn empty( this ) : bool
	{
		static_if( is_nullable )
		{
			return is_nullptr(value_);
		}
		else
		{
			return false;
		}
	}

	// Checked conversion to non-nullable from nullable.
	fn enable_if(is_nullable) try_to_non_nullable( this ) : shared_ptr_mt_base</T, false, is_mutable/>
	{
		halt if(empty());

		unsafe
		{
			atomic_inc( $>(counter_).use_count_total  );
			atomic_inc( $>(counter_).use_count_strong );
			return shared_ptr_mt_base</T, false, is_mutable/>( value_, counter_ );
		}
	}

	// Lock operations.

	fn enable_if(is_nullable && is_mutable) try_lock_mut( this ) : lock_mut_t  @( reference_notation::return_inner_references::param0 )
	{
		halt if( empty() );
		return unsafe( lock_mut_unchecked() );
	}

	fn enable_if(is_nullable) try_lock_imut( this ) : lock_imut_t @( reference_notation::return_inner_references::param0 )
	{
		halt if( empty() );
		return unsafe( lock_imut_unchecked() );
	}

	fn enable_if(!is_nullable && is_mutable) lock_mut( this ) : lock_mut_t @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( lock_mut_unchecked() );
	}

	fn enable_if(!is_nullable) lock_imut( this ) : lock_imut_t @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( lock_imut_unchecked() );
	}

	fn lock_mut_unchecked ( this ) unsafe : lock_mut_t @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( lock_mut_t ( value_, counter_ ) );
	}

	fn lock_imut_unchecked( this ) unsafe : lock_imut_t @( reference_notation::return_inner_references::param0 )
	{
		return unsafe( lock_imut_t( value_, counter_ ) );
	}

	// Compare.

	op==( this_type& l, this_type& r ) : bool
	{
		return l.value_ == r.value_;
	}

	// Hashing.
	template</type Hasher/>
	fn hash( this, Hasher &mut hasher )
	{
		// Just hash pointer value.
		apply_value_to_hasher( hasher, value_ );
	}

public: // Methods for internal usage.

	fn get_value_ptr( this ) unsafe : $(T)
	{
		return value_;
	}

	fn get_counter_ptr( this ) unsafe : $(shared_ptr_mt_impl::counter)
	{
		return counter_;
	}

	// Counter must be incremented before this constructor call.
	fn constructor( $(T) value_ptr, $(shared_ptr_mt_impl::counter) counter_ptr ) unsafe
	( value_(value_ptr), counter_(counter_ptr) )
	{}

private:
	type this_type= shared_ptr_mt_base</ T, is_nullable, is_mutable />;
	type lock_mut_t = shared_ptr_mt_impl::lock_mut </T/>;
	type lock_imut_t= shared_ptr_mt_impl::lock_imut</T/>;

private:
	$(T) value_= zero_init;
	$(shared_ptr_mt_impl::counter) counter_ = zero_init;
}

template</ type T, bool is_mutable />
class weak_ptr_mt_base
{
public:
	fn constructor()= default;

	// Construct from non-nullable immutable shared_ptr_mt.
	fn enable_if(!is_mutable) conversion_constructor( shared_ptr_mt_base</ T, false, false/>& ptr )
	{
		unsafe
		{
			value_= ptr.get_value_ptr();
			counter_= ptr.get_counter_ptr();
			atomic_inc( $>(counter_).use_count_total );
		}
	}

	// Construct from non-nullable mutable shared_ptr_mt.
	fn conversion_constructor( shared_ptr_mt_base</ T, false, true/>& ptr )
	{
		unsafe
		{
			value_= ptr.get_value_ptr();
			counter_= ptr.get_counter_ptr();
			atomic_inc( $>(counter_).use_count_total );
		}
	}

	// Construct from nullable immutable shared_ptr_mt.
	fn enable_if(!is_mutable) conversion_constructor( shared_ptr_mt_base</ T, true, false/>& ptr )
	{
		unsafe
		{
			value_= ptr.get_value_ptr();
			counter_= ptr.get_counter_ptr();
			if( !is_nullptr(counter_) )
			{
				atomic_inc( $>(counter_).use_count_total );
			}
		}
	}

	// Construct from nullable mutable shared_ptr_mt.
	fn conversion_constructor( shared_ptr_mt_base</ T, true, true/>& ptr )
	{
		unsafe
		{
			value_= ptr.get_value_ptr();
			counter_= ptr.get_counter_ptr();
			if( !is_nullptr(counter_) )
			{
				atomic_inc( $>(counter_).use_count_total );
			}
		}
	}

	// Construct from weak_ptr_mt of compatible type.
	template</ type U />
	fn enable_if( !same_type</T, U/> && is_ancestor_for</T, U/>() )
	conversion_constructor( mut this, weak_ptr_mt_base</U, is_mutable/>& ptr )
	{
		unsafe
		{
			if( !is_nullptr(ptr.get_value_ptr()) )
			{
				value_= $<( cast_ref</T/>( $>( ptr.get_value_ptr() ) ) );
				counter_= ptr.get_counter_ptr();
				atomic_inc( $>(counter_).use_count_total );
			}
		}
	}

	// Copy constructor.
	fn constructor( mut this, this_type &imut other )
	{
		unsafe
		{
			value_= other.get_value_ptr();
			counter_= other.get_counter_ptr();
			if( !is_nullptr(counter_) )
			{
				atomic_inc( $>(counter_).use_count_total );
			}
		}
	}

	fn destructor()
	{
		unsafe
		{
			if( !is_nullptr(counter_) )
			{
				if( atomic_dec( $>(counter_).use_count_total ) == 1u )
				{
					rwlock_impl::rwlock_destroy( $>(counter_).rwlock );
					// Free counter together with initial allocated object.
					memory_free( ptr_cast_to_byte8( counter_) );
				}
			}
		}
	}

	op=( mut this, this_type &imut other )
	{
		// Call copy constructor for other and move-assign copy to this.
		this= this_type(other);
	}

	fn reset( mut this )
	{
		// Move value ot out "this", call destructor for it, construct default value in place of "this".
		take(this);
	}

	fn lock( this ) : shared_ptr_mt_base</T, true, is_mutable/>
	{
		unsafe
		{
			if( is_nullptr(counter_) )
			{
				return shared_ptr_mt_base</T, true, is_mutable/>();
			}

			atomic_inc( $>(counter_).use_count_total );

			var u32 mut count= atomic_read( $>(counter_).use_count_strong );
			loop
			{
				if( count == 0u )
				{
					atomic_dec( $>(counter_).use_count_total );
					return shared_ptr_mt_base</T, true, is_mutable/>();
				}

				if( atomic_compare_exchange_weak(
						$>(counter_).use_count_strong,
						count,
						count + 1u ) )
				{
					break;
				}
			}

			return shared_ptr_mt_base</T, true, is_mutable/>( value_, counter_ );
		}
	}

public: // Methods for internal usage.
	fn get_value_ptr( this ) unsafe : $(T)
	{
		return value_;
	}

	fn get_counter_ptr( this ) unsafe : $(shared_ptr_mt_impl::counter)
	{
		return counter_;
	}

private:
	type this_type= weak_ptr_mt_base</ T, is_mutable />;
	type lock_mut_t = shared_ptr_mt_impl::lock_mut </T/>;
	type lock_imut_t= shared_ptr_mt_impl::lock_imut</T/>;

private:
	$(T) value_= zero_init;
	$(shared_ptr_mt_impl::counter) counter_= zero_init;
}

// All necessary type aliases.

template</type T/> type shared_ptr_mt_imut= shared_ptr_mt_base</T, false, false/>;
template</type T/> type shared_ptr_mt_mut= shared_ptr_mt_base</T, false, true/>;
template</type T/> type shared_ptr_mt_nullable_imut= shared_ptr_mt_base</T, true, false/>;
template</type T/> type shared_ptr_mt_nullable_mut= shared_ptr_mt_base</T, true, true/>;
template</type T/> type weak_ptr_mt_imut= weak_ptr_mt_base</T, false/>;
template</type T/> type weak_ptr_mt_mut= weak_ptr_mt_base</T, true/>;

// Helper function for shared_ptr_mt construction.
template</type T/>
fn make_shared_ptr_mt( T mut value ) : shared_ptr_mt_mut</T/>
{
	return shared_ptr_mt_mut</T/>( move(value) );
}

namespace shared_ptr_mt_impl
{

struct counter
{
	u32 use_count_total;
	u32 use_count_strong; // always <= use_count_total
	rwlock_impl::rwlock rwlock;
}

template</ type T />
class lock_mut nodiscard
{
public:
	fn constructor( mut this, $(T) v, $(counter) c ) unsafe
		( value_(v), counter_(c) )
	{
		unsafe( rwlock_impl::rwlock_lock_exclusive( $>(counter_).rwlock ) );
	}

	fn destructor()
	{
		unsafe( rwlock_impl::rwlock_unlock_exclusive( $>(counter_).rwlock ) );
	}

	// Mark reference by "this" tag, instead of inner tag, for prevention of "lock" destruction.
	fn deref( mut this ) : T & mut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>( value_ ) );
	}

private:
	// Use this tag in order to mark lock class as reference-contained.
	// It contains actually a logical reference to source shared_ptr - "counter".
	ReferenceContainerTag</ counter, false /> shared_ptr_ref_tag_;

	$(T) value_;
	$(counter) counter_= zero_init;
}

template</ type T />
class lock_imut nodiscard
{
public:
	fn constructor( mut this, $(T) v, $(counter) c)  unsafe
		( value_(v), counter_(c) )
	{
		unsafe( rwlock_impl::rwlock_lock_shared( $>(counter_).rwlock ) );
	}

	fn destructor()
	{
		unsafe( rwlock_impl::rwlock_unlock_shared( $>(counter_).rwlock ) );
	}

	// Mark reference by "this" tag, instead of inner tag, for prevention of "lock" destruction.
	fn deref( this ) : T & imut @( reference_notation::return_references::param0 )
	{
		return unsafe( $>(value_) );
	}

private:
	// Use this tag in order to mark lock class as reference-contained.
	// It contains actually a logical reference to source shared_ptr - "counter".
	ReferenceContainerTag</ counter, false /> shared_ptr_ref_tag_;

	$(T) value_;
	$(counter) counter_= zero_init;
}

} // namespace shared_ptr_mt_impl

} // namespace ust

import "random_access_range.u"

namespace ust
{

// Some sort of hasher.
// All alternative implementations must have the same interface (implement the same methods).
// The hashing algorithm implemented by this class is determenistic, but isn't portable and may be changed.
// If one needs portable stable hashing, some other hasher implementation should be used instead.
//
// TODO - speed-up and improve this implementation.
//
class default_hasher
{
public:
	// Get current accumulated state.
	fn get( this ) : size_type
	{
		// TODO - maybe finalize this hash somehow?
		return state_;
	}

	// Reset internal state (discarding all previously accumulated values).
	fn reset( mut this )
	{
		state_= 0s;
	}

	// op() overloading for applying and accumulating scalars.

	op()( mut this, void v )
	{
		write( 0s );
		return v;
	}

	op()( mut this, bool b )
	{
		write( ( b ? 1s : 0s ) );
	}

	op()( mut this, i8 x )
	{
		write( size_type(x) );
	}

	op()( mut this, u8 x )
	{
		write( size_type(x) );
	}

	op()( mut this, i16 x )
	{
		write( size_type(x) );
	}

	op()( mut this, u16 x )
	{
		write( size_type(x) );
	}

	op()( mut this, i32 x )
	{
		write( size_type(x) );
	}

	op()( mut this, u32 x )
	{
		write( size_type(x) );
	}

	op()( mut this, i64 x )
	{
		this( u64(x) );
	}

	op()( mut this, u64 x )
	{
		static_if( typeinfo</i64/>.size_of == typeinfo</size_type/>.size_of )
		{
			write( size_type(x) );
		}
		else
		{
			// Write low and high parts separatly if size_type is 32-bit.
			write( size_type(x) );
			write( size_type( x >> 32u ) );
		}
	}

	op()( mut this, i128 x )
	{
		this( u128(x) );
	}

	op()( mut this, u128 x )
	{
		// Write 128-bit values as pair of 64-bit values.
		this( u64(x) );
		this( u64( x >> 64u ) );
	}

	op()( mut this, size_type x )
	{
		write( x );
	}

	op()( mut this, ssize_type x )
	{
		write( size_type(x) );
	}

	op()( mut this, char8 x )
	{
		write( size_type(x) );
	}

	op()( mut this, char16 x )
	{
		write( size_type(x) );
	}

	op()( mut this, char32 x )
	{
		write( size_type(x) );
	}

	op()( mut this, byte8 x )
	{
		write( size_type(u8(x)) );
	}

	op()( mut this, byte16 x )
	{
		write( size_type(u16(x)) );
	}

	op()( mut this, byte32 x )
	{
		write( size_type(u32(x)) );
	}

	op()( mut this, byte64 x )
	{
		static_if( typeinfo</byte64/>.size_of == typeinfo</size_type/>.size_of )
		{
			write( size_type(x) );
		}
		else
		{
			write( size_type( u64(x) ) );
			write( size_type( u64(x) >> 32u ) );
		}
	}

	op()( mut this, byte128 x )
	{
		this( u128(x) );
	}

	op()( mut this, f32 mut x )
	{
		// Fix hashing of negative signed zero - hash it as positive zero.
		if( x == 0.0f )
		{
			x= 0.0f;
		}
		// Hash just bits of a floating point value. It's mostly fine, except NaNs, but they can't work normally anyway.
		this( byte32(x) );
	}

	op()( mut this, f64 mut x )
	{
		// Fix hashing of negative signed zero - hash it as positive zero.
		if( x == 0.0 )
		{
			x= 0.0;
		}
		// Hash just bits of a floating point value. It's mostly fine, except NaNs, but they can't work normally anyway.
		this( byte64(x) );
	}

	// It's necessary to implement methods for byte-ranges, since it may be more effective to hash the whole range, instead to hashing values one by one.

	op()( mut this, array_view_imut</byte8/> range )
	{
		static_if( typeinfo</size_type/>.size_of == 4s )
		{
			// Read and hash whole words.
			var size_type size_rounded= range.size() & (~3s);
			unsafe  // Because of unchecked indexing.
			{
				for( var size_type mut i= 0s; i < size_rounded; i+= 4s )
				{
					// Perform reading of each byte individually, because they may be unaligned in order to be able to perform whole word readings.
					var size_type word=
						( size_type( u8( range.index_unchecked( i + 0s ) ) ) <<  0u ) |
						( size_type( u8( range.index_unchecked( i + 1s ) ) ) <<  8u ) |
						( size_type( u8( range.index_unchecked( i + 2s ) ) ) << 16u ) |
						( size_type( u8( range.index_unchecked( i + 3s ) ) ) << 24u );

					safe( write( word ) );
				}
			}

			// Read and hash tail.
			if( size_rounded < range.size() )
			{
				var size_type mut word= 0s;
				unsafe // Because of unchecked indexing.
				{
					for( var size_type mut i= size_rounded; i < range.size(); ++i )
					{
						word <<= 8u;
						word |= size_type( u8( range.index_unchecked( i ) ) );
					}
				}

				write( word );
			}
		}
		else
		{
			// Read and hash whole words.
			var size_type size_rounded= range.size() & (~7s);
			unsafe // Because of unchecked indexing.
			{
				for( var size_type mut i= 0s; i < size_rounded; i+= 8s )
				{
					// Perform reading of each byte individually, because they may be unaligned in order to be able to perform whole word readings.
					var size_type word=
						( size_type( u8( range.index_unchecked( i + 0s ) ) ) <<  0u ) |
						( size_type( u8( range.index_unchecked( i + 1s ) ) ) <<  8u ) |
						( size_type( u8( range.index_unchecked( i + 2s ) ) ) << 16u ) |
						( size_type( u8( range.index_unchecked( i + 3s ) ) ) << 24u ) |
						( size_type( u8( range.index_unchecked( i + 4s ) ) ) << 32u ) |
						( size_type( u8( range.index_unchecked( i + 5s ) ) ) << 40u ) |
						( size_type( u8( range.index_unchecked( i + 6s ) ) ) << 48u ) |
						( size_type( u8( range.index_unchecked( i + 7s ) ) ) << 56u );

					safe( write( word ) );
				}
			}

			// Read and hash tail.
			if( size_rounded < range.size() )
			{
				var size_type mut word= 0s;
				unsafe // Because of unchecked indexing.
				{
					for( var size_type mut i= size_rounded; i < range.size(); ++i )
					{
						word <<= 8u;
						word |= size_type( u8( range.index_unchecked( i ) ) );
					}
				}

				write( word );
			}
		}
	}

	op()( mut this, array_view_imut</byte16/> range )
	{
		static_if( typeinfo</size_type/>.size_of == 4s )
		{
			// Read and hash whole words.
			var size_type size_rounded= range.size() & (~1s);
			unsafe // Because of unchecked indexing.
			{
				for( var size_type mut i= 0s; i < size_rounded; i+= 2s )
				{
					// Perform reading of each byte16 element individually, because they may be unaligned in order to be able to perform whole word readings.
					var size_type word=
						( size_type( u16( range.index_unchecked( i + 0s ) ) ) <<  0u ) |
						( size_type( u16( range.index_unchecked( i + 1s ) ) ) << 16u );

					safe( write( word ) );
				}
			}

			// Read and hash tail.
			if( size_rounded < range.size() )
			{
				write( size_type( u16( range.back() ) ) );
			}
		}
		else
		{
			// Read and hash whole words.
			var size_type size_rounded= range.size() & (~3s);
			unsafe // Because of unchecked indexing.
			{
				for( var size_type mut i= 0s; i < size_rounded; i+= 4s )
				{
					// Perform reading of each byte16 element individually, because they may be unaligned in order to be able to perform whole word readings.
					var size_type word=
						( size_type( u16( range.index_unchecked( i + 0s ) ) ) <<  0u ) |
						( size_type( u16( range.index_unchecked( i + 1s ) ) ) << 16u ) |
						( size_type( u16( range.index_unchecked( i + 2s ) ) ) << 32u ) |
						( size_type( u16( range.index_unchecked( i + 3s ) ) ) << 48u );

					safe( write( word ) );
				}
			}

			// Read and hash tail.
			if( size_rounded < range.size() )
			{
				var size_type mut word= 0s;
				unsafe // Because of unchecked indexing.
				{
					for( var size_type mut i= size_rounded; i < range.size(); ++i )
					{
						word <<= 16u;
						word |= size_type( u16( range.index_unchecked( i ) ) );
					}
				}

				write( word );
			}
		}
	}

	op()( mut this, array_view_imut</byte32/> range )
	{
		static_if( typeinfo</size_type/>.size_of == 4s )
		{
			foreach( b : range )
			{
				write( size_type(b) );
			}
		}
		else
		{
			// Read and hash whole words.
			var size_type size_rounded= range.size() & (~1s);
			unsafe // Because of unchecked indexing.
			{
				for( var size_type mut i= 0s; i < size_rounded; i+= 2s )
				{
					// Perform reading of each byte32 element individually, because they may be unaligned in order to be able to perform whole word readings.
					var size_type word=
						( size_type( u32( range.index_unchecked( i + 0s ) ) ) <<  0u ) |
						( size_type( u32( range.index_unchecked( i + 1s ) ) ) << 32u );

					safe( write( word ) );
				}
			}

			// Read and hash tail.
			if( size_rounded < range.size() )
			{
				write( size_type( u32( range.back() ) ) );
			}
		}
	}

	op()( mut this, array_view_imut</byte64/> range )
	{
		foreach( b : range )
		{
			this( b );
		}
	}

	op()( mut this, array_view_imut</byte128/> range )
	{
		foreach( b : range )
		{
			this( b );
		}
	}

private:
	fn write( mut this, size_type mut x )
	{
		static_if( typeinfo</size_type/>.size_of == 4s )
		{
			// Some sort of MurmurHash3.
			// Strictly-speaking it's not MurmurHash3, but only its inner loop.

			x*= size_type( 0xcc9e2d51u );
			x= ( x << 15u ) | ( x >> 17u );
			x*= size_type(0x1b873593u);

			state_ ^= x;
			state_= ( state_ << 13u ) | ( state_ >> 19u );
			state_= state_ * 5s + size_type( 0xe6546b64u );
		}
		else
		{
			// MurmurHash2, 64-bit version, slightly modified.
			// Strictly-speaking it's not MurmurHash2, but only its inner loop.

			var size_type m( 0xc6a4a7935bd1e995u );
			var u32 r = 47u;

			x *= m;
			x ^= x >> r;
			x *= m;

			state_ ^= x;
			state_ *= m;
			state_ += size_type( 0xe6546b64 );
		}
	}

private:
	size_type state_= 0s;
}

} // namespace ust
